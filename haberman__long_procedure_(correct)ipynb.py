# -*- coding: utf-8 -*-
"""haberman _long procedure (correct)ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kvsOASfK7fVNAW80EG3IBddFnuJx6lku
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

df=pd.read_csv('/content/drive/MyDrive/dataset/haber.csv')

df.head()

columns=['Age','Year_Of_Operation','Positive_Nodes','Status']

df=pd.read_csv('/content/drive/MyDrive/dataset/haber.csv',names=columns)

df.head(3000)

df.isnull().sum()

print(df.shape)

#df['Status'].value_counts()
df['Age'].value_counts()

slices=df['Status'].value_counts()

plt.pie(x=slices,labels=['Survived','Non_Survived'])

# Feature Age

print("Patient with lowest Age",df['Age'].min())

# patients with highest age
print("Patients with highest age",df['Age'].max())

print("Mean Age of the patient",df['Age'].mean())

#df['Status'].value_counts()
df['Age'].value_counts()

sns.histplot(x=df['Age'],label='Age',bins=9,kde=True)
plt.xlabel('Age')
plt.ylabel("Density(Patient Count)")
plt.title('Patient Age Distribution')
plt.xticks(ticks=range(25,85,5))
plt.yticks(ticks=range(0,90,10)) # doubt when we use Age.value_counts it is showing max no as 13 but this graph is showing bigger values

df['Status']=df['Status'].astype('int')
df=df.replace({'Status':{1:1,2:0}})
print(df['Status'].value_counts())

df

df.shape

x=df.iloc[:,0:3].values
y=df.iloc[:,3:].values
print(x.shape)
print(y.shape)

x_train=x[:250,]
x_test=x[250:,:]
y_train=y[:250,]
y_test=y[250:,]
print(x_test.shape)

u=np.mean(x,axis=0)
std=np.std(x,axis=0)

x_train=(x_train-u)/std
x_test=(x_test-u)/std
a=np.mean(y,axis=0)
b=np.std(y,axis=0)
y_train=(y_train-a)/b
y_test=(y_test-a)/b





print(x_train.std())

def sigmoid(x):
  return 1.0/(1.0 +np.exp(-x))

def hypothesis(x,theta):
  return sigmoid(np.dot(x,theta))

def gradient(x,y,theta):
  h_theta=hypothesis(x,theta)
  grad=np.dot(x.T,(y-h_theta))
  return grad/x.shape[0]

def cost_function(x,y,theta):
  h_theta=hypothesis(x,theta)
  cost=np.mean(y*np.log(h_theta)+(1-y)*np.log(1-h_theta))
  cost=cost*-1
  return cost

def gradient_descent(x,y,learning_rate=0.01,max_steps=1000):
  m,n=x.shape 
  theta=np.zeros((n,1))
  cost_epoch=[]
  for i in range(max_steps):
    grad=gradient(x,y,theta)
    e=cost_function(x,y,theta)
    cost_epoch.append(e)
    theta=theta+learning_rate*grad
  return (theta,cost_epoch)

ones=np.ones((x_train.shape[0],1))
x_train=np.hstack((ones,x_train))
y_train=y_train.reshape((-1,1))

theta,cost_epoch=gradient_descent(x_train,y_train)

theta

cost_epoch

ones=np.ones((x_test.shape[0],1))
x_test=np.hstack((ones,x_test))
y_test=y_test.reshape((-1,1))

plt.plot(cost_epoch)

x1 = np.arange(-3, 5)

x2 = -(theta[0] * theta[1]*x1)/theta[2]

plt.scatter(x_train[:, 1], x_train[:, 2], c = y_train)
plt.plot(x1, x2)

def predict(X, theta):
    h_theta = hypothesis(X, theta)
    output = np.zeros(h_theta.shape)

    output[h_theta>0.5] = 1
    output = output.astype('int')

    return output

train_preds = predict(x_train, theta)
test_preds = predict(x_test, theta)

for i in range(len(test_preds)):
    print(test_preds[i], y_test[i])

def accuracy(preds, labels):
    labels = labels.astype('int')
    print(np.sum(preds == labels)/labels.shape[0])

accuracy(train_preds,y_train)

accuracy(test_preds, y_test)

